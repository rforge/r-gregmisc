\section{Methods}

\begin{verbatim}
$Id$
\end{verbatim}

  \subsection{The Pathway}
  
    While we examined several pathway structures, in this paper we present the results for a 
    5-reaction sequence that is sampled at 12, 16, or 25 time points 
    with 3 replicates at each time point. The units for time and concentration are arbitrary; for the purpose of illustration we will use minutes as the unit of time and micromoles/liter ($\mu$M) as the unit of concentration. 
<<data, echo=F, eval=F>>=1
# gillespie.out - output from the Gillespie simulation
source('R/getPaperData.R')
source('R/getPaperRates.R')
# trim the Gillespie output
rawdata <- read.table('data/gillespie.out',header=T)[2528:7160,]
# sampling times:
times16 <- c(16:19,20.5,21,seq(22,40,2))
# generating the MCMC input file input16.dat
data <- getPaperData(rawdata,20,times16,npoints=3, varVector=c(50,50,50,50,50))
rates <- getPaperRates(data,1:12,13:48)
temp <- matrix(0,nrow=16,ncol=11)
for (i in 1:16) temp[i,] <- (rates[3*i,]+rates[3*i-1,]+rates[3*i-2,])/3
colnames(temp) <- colnames(rates)
write.table(temp,file="data/input16.dat",row.names=F,quote=F)
@  
    The sequence is
    \begin{reaction} source \yields R1 \yields R2 \yields
      R3 \yields R4 \yields R5 \yields sink \end{reaction}
    and the equations that were used in the Gillespie simulation
    are
    \begin{chemarray*}
      source &\yields^{k_1}& R1\\
      R1 + E1 &\eqbm^{k_2}_{k_3}& R1E1 \yields^{k_4} R2 + E1\\
      R2 + E2 &\eqbm^{k_5}_{k_6}& R2E2 \yields^{k_7} R3 + E2\\
      R3 + E3 &\eqbm^{k_8}_{k_9}& R3E3 \yields^{k_{10}} R4 + E3\\
      R4 + E4 &\eqbm^{k_{11}}_{k_{12}}& R4E4 \yields^{k_{13}} R5 + E4\\
      R5 &\yields^{k_{14}}& sink
    \end{chemarray*}

  \subsection{The Experiment}

    To estimate the kinetic parameters of the enzymes in a sequence of
    reactions, we need to find expressions of the form
    \[
    \text{reaction velocity} = f(\mathit{[substrate],[enzyme]})
    \]
    These expressions are found by perturbing a reaction network
    and observing the metabolite concentrations as the system relaxes
    back to a steady state. Specifically, the Gillespie simulation was 
    run with the initial values for the reactant concentrations until 
    a steady state was achieved. The steady state appeared to have 
    been reached by \textit{time} = 15 min. At \textit{time} = 20 min
    the concentration of $R1$ was increased to 10000 $\mu$M. At the indicated time points the metabolite concentrations are
    measured, and each reaction velocity is calculated as the
    slope of the curve of metabolite concentration as a function of
    time.

    \subsection{The data}

    For the 5-reaction sequence of reactions 
    the perturbation of $R1$ at $time = 20$ results in the time
    courses plotted in Figure~\ref{pulse}.
    For each time point there are 5 values for the reactant
    concentrations and 5 values for the estimated reaction rates.
    The data at each time point was generated by sampling 3 values for
    each reactant from a normal distribution centered on the Gillespie
    output and with a standard deviation of 50 $\mu$M, i.e., the 
    measurement error is 50 $\mu$M. Three sets of time
    points containing 12,16, and 25 points were used.
    The reaction velocities were estimated with the
    \textit{smooth.spline()} function in \textit{R}~\cite{R}.

    Note that we are dealing with two different types of distribution
    here. One is the distribution of data values that arises from
    experimental error and the other is the distribution of model
    parameter values that are generated by MCMC simulation. The
    uncertainty in the parameter values is reduced by increasing the
    number of data points within the fixed time interval
    (Figure~\ref{converged}) and by decreasing experimental error but
    in general there is not a closed form solution for these
    relationships.

  \subsection{The Biochemical Model}

    Individual reactions were fit using the Michaelis-Menten
    equation \cite{Michaelis13} for individual enzymes. This is a
    reaction of the form
    \begin{reaction}\label{eqnForm}
      E + S \eqbm^{k_1}_{k_2} ES \yields^{k_3} E + P
    \end{reaction}
    where
    \begin{eqnarray*}
      S &=& \text{substrate concentration}\\
      E &=& \text{free enzyme concentration}\\
      ES &=& \text{concentration of the enzyme-substrate complex}\\
      P &=& \text{product concentration}
    \end{eqnarray*}
    It has been shown \cite{Briggs25} that in a steady state the rate
    of the reaction $v$ is
    \begin{equation}
    v = \frac{V_{max}S}{K_m+S} 
    \end{equation}
    where
    \begin{eqnarray*}
      V_{max} &=& (E+ES)k_3 = \text{maximum reaction velocity}\\[2mm]
      K_m &=& \frac{k_2+k_3}{k_1} = \text{substrate concentration at
      half-maximal velocity}
    \end{eqnarray*}
    This form of the equation is very useful because $v$ and $S$ are
    usually measureable and $V_{max}$ and $K_m$ can be obtained by
    fitting  equation 3 to the data. In contrast, modeling $v$ as a
    function of the individual rate constants is less useful because
    that requires the measurement of the concentration of the
    enzyme-substrate complex which is technically difficult.

    In this application we are dealing with sequences of reactions
    which are not in a steady state, so we cannot use the
    Michaelis-Menten equation directly. Instead, we use equations
    of the form
    \begin{equation}
    v = \frac{aS}{b+S} - \frac{cP}{d+P} 
  \end{equation}
    
    The coefficients $a$, $b$, $c$, and $d$ in the equations can be estimated with data
    obtained following a change in the concentration of one
    of the reactants. For example, four equations
    can be fit to the data plotted in Figure~\ref{pulse}:
    \begin{align*}
      \deriv{R2}{t} &= v_2 = \frac{d_1R1}{d_2 + R1} -
      \frac{d_3R2}{d_4 + R2}\\[5mm]
      \deriv{R3}{t} &= v_3 = \frac{d_3R2}{d_4 + R2} - \frac{d_5R3}{d_6
      + R3}\\[5mm]
      \deriv{R4}{t} &= v_4 = \frac{d_5R3}{d_6 + R3} - \frac{d_7R4}{d_8
      + R4}\\[5mm]
      \deriv{R5}{t} &= v_5 = \frac{d_7R4}{d_8 + R4} - d_9R5
    \end{align*}

    \SweaveInput{figure2}

  \subsection{Statistical Models}

    The statistical model of the parameters is the product of a prior
    distribution and a likelihood distribution, where the prior
    expresses our current understanding of the parameter values (via
    probability distributions) and the likelihood is our model for the
    probability of the parameters given the data.
  
    The parameters to be estimated, i.e., the modes and
    the standard deviation of the rate constant distributions,
    are necessarily non-negative and have maximum values that are
    bounded by the physical nature of the systems. We model this using
    a scaled $\chi^2$ distribution with $5$ degrees of freedom:  
    \[
    \frac{3d_i}{\mu_i} \sim \chi_5^2
    \]
    where the $\mu_i$ are the estimates of the parameter values before
    any data are collected.  This distribution is strictly positive,
    places the mode of $d_i$s at $\mu_i$, and has standard deviation
    of $3*sqrt(10)*\mu_i \approx \mu_i$.

    For this  situation, we selected $\mu_i \equiv 50$ units for
    the rate constants and $\sigma^2 \equiv 2$ for the data standard
    deviation of the estimates. (see figure \ref{priorPlot})

    \SweaveInput{priorPlot}
  
    The likelihood is the probability of the
    estimated reaction velocities $v_j$ given the model.
    The error in the velocity estimates is assumed
    to be $N(0,\sigma^2)$ so that the reaction velocities $v_j$ have a
    normal distribution:
    \[
    v_j \sim N(\psi_j,\sigma^2)
    \]
    where
    \[
    \psi_j = \frac{aS_j}{b+S_j} - \frac{cP_j}{d+P_j} 
    \]
    The observed data consist of values for $v_j$, $S_j$, and $P_j$. For
    example, for the 5-reaction model we have
    \begin{align*}
      v_2 &\sim N\left(\frac{d_1R1}{d_2+R1} -
      \frac{d_3R2}{d_4+R2}, \;\; \sigma^2\right)\\
      v_3 &\sim N\left(\frac{d_3R2}{d_4+R2} -
      \frac{d_5R3}{d_6+R3}, \;\; \sigma^2\right)\\
      v_4 &\sim N\left(\frac{d_5R3}{d_6+R3} -
      \frac{d_7R4}{d_8+R4}, \;\; \sigma^2\right)\\
      \intertext{and}
      v_5 &\sim N\left(\frac{d_7R4}{d_8+R4} - d_9R5, \;\; \sigma^2\right)
    \end{align*}
    In this case there are 10 parameters to be estimated: the 9
    coefficients $d_1$ -- $d_9$ and $\sigma^2$.

    \subsubsection{MCMC sampling algorithms}

    The efficiency of a MCMC simulation
    \cite{Metropolis53,Hastings70} depends heavily on the
    method used to find candidate points to add to the Markov chain
    [$q(\cdot |\cdot)$]. 
    For this reason different methods have been developed to increase the efficiency of sampling from the sample space. We have used 
    three algorithms from the Hydra
    library \cite{Hydra}:
     component-wise Metropolis, all-components Metropolis, 
    and Normal Kernel Coupler \cite{Warnes00}.

    The component-wise Metropolis and the all-components Metropolis
    algorithms both operate on single Markov chains. The
    component-wise algorithm (Figure~\ref{1comp}) generates candidate points by
    changing the value of only one component (dimension) of the current state at
    each iteration by sampling from a univariate normal distribution
    centered at the current point.

    \SweaveInput{figure3}

    The all-components algorithm (Figure~\ref{allComp}) changes all the components
    simultaneously by sampling from a multivariate normal distribution
    centered at the current point.
    The component-wise Metropolis algorithm has the advantage of
    simplicity but may move very slowly if the components are highly
    correlated. The all-components Metropolis avoids the problems with
    correlation, if an appropriate covariance matrix is supplied, by updating all
    components simultaneously but may perform poorly in high
    dimensions. A problem with both of these algorithms
    is that if the component distributions have two or more modes
    separated by areas of low probability, the simulator can get stuck
    in the vicinity of one mode and fail to visit the other. The NKC algorithm
    is designed to avoid this difficulty.
    
    The Normal Kernel Coupler (NKC) algorithm operates on multiple chains, so
    that there are several ``current'' states. The NKC uses a normal
    kernel density estimate as the proposal distribution for choosing candidate
    states. The estimate is generated using the entire set of
    current points.  Since the algorithm
    uses the entire set of current points it can move over areas of
    low probability in the parameter space, especially if the user takes
    care to seed each mode with a few points in the starting set.

  \section{Convergence}
    
    The MCMC simulations are run until the Markov chains have reached
    stable distributions as assessed by the \textit{mcgibbsit()} algorithm
    \cite{Warnes00}. \textit{mcgibbsit()} calculates the number of
    iterations necessary to estimate a user-specfied quantile $q$ to
    within $\pm r$ with probability $s$, i.e.,
    \textit{mcgibbsit()} indicates when the MCMC sampler has run long
    enough to provide good confidence interval estimates. The defaults, which are used
    in this paper, are $q=0.025$, $r=0.0125$, and $s=0.95$. These values generate estimates of the 2.5\% and 97.5\% quantiles to $\pm$ 0.0125 quantiles with probability 0.95.
