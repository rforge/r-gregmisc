\documentclass{article}
\usepackage{fullpage}

\title{
  Effective Simulation and Analysis of Biological Systems Through a
  \emph{Synergistic} Combination of Deterministic Mathematical
  Modeling and Bayesian Statistical Techniques.
}

\author{Gregory R. Warnes - Pfizer Inc. and Yale Univ. \\ 
        Robert Burrows - Northeast Biometrics }

\begin{document}

\maketitle

\abstract{ 
  
  Historically, there are two fundamentally different approaches to
  modeling biological systems: deterministic models of mechanism, and
  statistical models of association. Both techniques have played a
  fundamental role in learning about biology.  Unfortunately, neither
  technique is well equipped to handle the current deluge of
  ``high-resolution'' or ``omic'' molecular biology assays.
  
  In this presentation, I will briefly discuss why existing
  mathematical and statistical modeling techniques are individually
  insufficient for modeling complex biological systems.  I will then
  propose an approach that ``wraps'' deterministic models of mechanism
  within a Bayesian statistic model, and discuss the benefits this
  promises.  I will then describe our initial exploration of this
  approach, in which we demonstrate that we can effectively model
  several relatively simple metabolic pathway models which
  nevertheless defy standard mathematical and statistical approaches.
  
  I will conclude with an overview of our future plans, along with
  some musings on the promises and perils of this approach.

}
  
\section{introduction}

  Historically, there are two fundamentally different approaches to
  modeling biological systems: deterministic models of mechanism, and
  statistical models of association. Both techniques have played a
  fundamental role in learning about biology.
  
  Deterministic mathematical models (e.g systems of ordinary
  differential equations - ODEs) excel at representing direct
  causal/structural relationships and have dominated the study of
  very low-level systems containing relatively small numbers of
  relatively well-understood components.  These models have the
  advantage of being directly interpretable and permit simulation of
  the studied system to predict behavior, even in the absence of
  experimental data.
  
  Statistical modeling, on the other hand, allows ``black-box''
  analysis of complicated systems--based on association--when there are
  many interacting components, where the individual components cannot
  be measured, or where and the relationships among the components are
  not understood.  Statistical approaches can be resilient in the face
  of uncertainty, variability, and noise, provide the ability to
  quantify uncertainty, and even allow inference in the presence of
  considerable uncertainty about causal mechanism(s), provided that
  appropriate experimental data is available.
  
  With the advent of ``high-resolution'' or ``-omics'' molecular
  biology technologies such as RNA expression arrays, whole-genome SNP
  genotyping, NMR metabanomics, Mass-Spec proteomics, etc., we can now
  measure the behavior of very large numbers of low-level system
  components, many of which are directly interacting. Unfortunately,
  the analysis of these data sets, and the systems from which they are
  generated, cannot be effectively accomplished using either
  traditional modeling approach.
  
  For example, we know maddingly little about almost all of the
  components of our biological systems.  This prevents effective apply
  standard deterministic modeling approaches to even moderately sized
  systems.  Even when we have some concrete information about the
  relationships, e.g. rate constants, there is no clear mechanism for
  dealing with uncertainty about the values of these constants.  Nor
  is there a coherent mechanism for fitting realistic data, which
  necessarily contains noise and unobserved components, to standard
  deterministic models.
  
  In turn, standard frequentist statistical methods are centered
  around association and not causation, and hence are unable to
  effectively incorporate known causal relationships, or to
  effectively infer such relationships.  In addition frequentist
  techniques are also unable to integrate external information about
  model components.
  
  Further, despite workarounds, both techniques are are essentially
  overwhelmed by the sheer number of components we can now measure.
  
  The sometimes maddening discrepancy between high level of detail on
  the nature and interactions among some components and almost
  complete ignorance of the nature and interactions of other
  components, along with the sheer complexity of the systems we are
  attempting to study demands that develop methods which combine the
  traditional strengths of deterministic mathematical and associative
  statistical models, while overcoming the the weaknesses of each.
  
  It this talk, I will present an initial effort to develop a modeling
  approach that combines the strengths of both existing approaches by
  ``wrapping'' the components of standard deterministic mathematical
  models within a Bayesian statistical model. This approach promises
  several benefits:
  \begin{itemize}
  \item The ``Bayesian wrapping'' allows the deterministic
    mathematical model to be applied effectively in the presence of
    uncertainty, variability, and noise. I.e., this allows effective
    fitting of experimental data to the deterministic mathematical
    model.
  \item The deterministic mathematical model ``core'' permits
    components of the system where the true interactions are known to
    be appropriately modeled.
  \item The Bayesian approach makes it possible to quantify the
    evidence for or against hypothesized relationships -- and even to
    falsify ``known'' relationships!
  \item Bayesian priors on model parameters provide a straightforward
    method for making use of external information, even when this
    information is inconsistent between sources.
  \end{itemize}



\end{document}
